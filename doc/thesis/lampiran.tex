%-----------------------------------------------------------------------------%
\addChapter{Lampiran 1: Kode Sumber Program Utama}
%-----------------------------------------------------------------------------%

\chapter*{Lampiran 1: Kode Sumber Program Utama}

Kode sumber program utama (\textit{main program}) \verb|extract_triples.py|

\begin{lstlisting}[language=Python]
import os
import csv
import argparse
import subprocess
import numpy as np
import json
from sys import platform
from sklearn.externals import joblib
from sklearn.preprocessing import StandardScaler
from tripletools import (
    vectorize,
    parse_connlu_file,
    extract_triples_by_combinations,
    get_best_features
)
from pprint import pprint

# choose script based on OS (windows or *nix)
DEPPARSE_SCRIPT = 'bin' + os.sep + 'id-openie'
if platform == 'win32':
    DEPPARSE_SCRIPT += '.bat'


def write_json(triples, y, out):
    count = 0
    grouped = {}
    for i in range(y.shape[0]):
        if y[i] == 1:
            triple = triples[i]
            if triple[1] not in grouped:
                grouped[triple[1]] = {}
            if triple[2] not in grouped[triple[1]]:
                grouped[triple[1]][triple[2]] = {}
            if triple[3] not in grouped[triple[1]][triple[2]]:
                grouped[triple[1]][triple[2]][triple[3]] = {}
            count += 1
    out.write(json.dumps(grouped) + '\n')
    return count


def write_tsv(triples, y, out):
    writer = csv.writer(out, delimiter='\t', quoting=csv.QUOTE_NONE, quotechar='')
    count = 0
    for i in range(y.shape[0]):
        if y[i] == 1:
            writer.writerow(triples[i])
            count += 1
    return count


def extract(conllu_file, classifier, out, format='tsv', scaler=None):
    X = []
    triples = []
    for index, s, s_header in parse_connlu_file(conllu_file):
        for first, second, third, subj, pred, obj in extract_triples_by_combinations(s, s_header):
            X.append(vectorize(first, second, third))
            triples.append((first['sentence_id'], subj, pred, obj))
    X = np.array(X, dtype='float32')
    # apply best features selection
    X = X[:, get_best_features()]
    # scale if scaler is available
    if scaler:
        X = scaler.transform(X)
    y = classifier.predict(X)
    # write output
    if format == 'tsv':
        return write_tsv(triples, y, out)
    else:  # format == 'json'
        return write_json(triples, y, out)


if __name__ == '__main__':

    if os.path.isfile(DEPPARSE_SCRIPT):
        parser = argparse.ArgumentParser(description='Extract triples from Indonesian text')
        parser.add_argument('input_file', help='Input file containing 1 (one) Indonesian sentence per line')
        parser.add_argument('-m', '--model_file', help='Triples classifier model file', default='triples-classifier-model.pkl')
        parser.add_argument('-s', '--scaler_file', help='Triples classifier scaler file', default='triples-classifier-scaler.pkl')
        parser.add_argument('-o', '--output_file', help='Output file containing triples')
        parser.add_argument('-f', '--output_format', help='Output file format', choices=['json', 'tsv'], default='json')
        args = parser.parse_args()
        args.output_file = args.output_file if args.output_file else 'triples.' + args.output_format

        # dependency parsing
        print('Parsing dependency tree..')
        depparse_output = os.path.basename(args.input_file) + '.conllu'
        subprocess.call([DEPPARSE_SCRIPT, '-f', args.input_file])

        # extract triples
        classifier = joblib.load(args.model_file)
        scaler = joblib.load(args.scaler_file)
        with open(args.output_file, 'wb') as out:
            count = extract(depparse_output, classifier, out, args.output_format, scaler=scaler)

        print('{} triple(s) extracted'.format(count))
        print('Triples saved in ' + args.output_file)
    else:
        print('File not found: ' + DEPPARSE_SCRIPT)
\end{lstlisting}

%-----------------------------------------------------------------------------%
\addChapter{Lampiran 2: Kode Sumber \textit{NLP Pipeline}}
%-----------------------------------------------------------------------------%

\chapter*{Lampiran 2: Kode Sumber \textit{NLP Pipeline}}

Kode sumber utama \textit{NLP pipeline}: \verb|DepdendencyParser.java|

\begin{lstlisting}[language=Java]
package id.nlp.depparser;

import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.pipeline.*;
import edu.stanford.nlp.semgraph.SemanticGraph;
import edu.stanford.nlp.semgraph.SemanticGraphCoreAnnotations;
import edu.stanford.nlp.trees.ud.CoNLLUDocumentWriter;
import edu.stanford.nlp.trees.ud.ExtendedCoNLLUDocumentWriter;
import edu.stanford.nlp.util.CoreMap;
import edu.stanford.nlp.util.PropertiesUtils;
import net.sourceforge.argparse4j.ArgumentParsers;
import net.sourceforge.argparse4j.inf.ArgumentParser;
import net.sourceforge.argparse4j.inf.ArgumentParserException;
import net.sourceforge.argparse4j.inf.Namespace;

import java.io.File;
import java.io.IOException;
import java.sql.SQLException;
import java.util.ArrayList;
import java.util.List;
import java.util.Properties;

import static edu.stanford.nlp.pipeline.Annotator.*;

public class DependencyParser {

    static final String TAGGER_MODEL = "tagger-id.universal.model";
    static final String NER_MODEL = "ner-id.model.ser.gz";
    static final String PARSER_MODEL = "parser-id.conllu.model.gz";
    static final int NUM_THREADS = 1;
    static final String OUTPUT_FORMAT = "conllu";

    AnnotatorPool annotatorPool;
    Properties props;
    StanfordCoreNLP pipeline;

    public DependencyParser() throws SQLException, IOException, ClassNotFoundException {
        this(TAGGER_MODEL, NER_MODEL, PARSER_MODEL, NUM_THREADS);
    }

    public DependencyParser(
            String taggerModel,
            String nerModel,
            String parserModel,
            int numThreads
    ) throws SQLException, IOException, ClassNotFoundException {

        // Create the Stanford CoreNLP pipeline
        this.props = PropertiesUtils.asProperties(
                "annotators", "tokenize,ssplit,pos,lemma,ner,depparse",
                "ner.model", nerModel,
                "ner.useSUTime", "false",
                "pos.model", taggerModel,
                "depparse.model", parserModel,
                "splitter.nomodel", "true",
                "ignore_affinity", "true",
                "outputFormat", OUTPUT_FORMAT,
                "threads", String.valueOf(numThreads)
        );

        // Create annotator pools
        this.annotatorPool = new AnnotatorPool();
        AnnotatorImplementations annotatorImplementations = new IndonesianAnnotatorImplementations();
        annotatorPool.register(STANFORD_TOKENIZE, AnnotatorFactories.tokenize(props, annotatorImplementations));
        annotatorPool.register(STANFORD_SSPLIT, AnnotatorFactories.sentenceSplit(props, annotatorImplementations));
        annotatorPool.register(STANFORD_POS, AnnotatorFactories.posTag(props, annotatorImplementations));
        annotatorPool.register(STANFORD_LEMMA, AnnotatorFactories.lemma(props, annotatorImplementations));
        annotatorPool.register(STANFORD_NER, AnnotatorFactories.nerTag(props, annotatorImplementations));
        annotatorPool.register(STANFORD_DEPENDENCIES, AnnotatorFactories.dependencies(props, annotatorImplementations));

        // Create pipeline
        this.pipeline = new IndonesianStanfordCoreNLP(this.props, annotatorPool);
    }

    /**
     * Parse text
     * @param text
     * @return
     */
    public String parse(String text) {
        StringBuilder result = new StringBuilder();
        Annotation doc = pipeline.process(text);
        List<CoreMap> sentences = doc.get(CoreAnnotations.SentencesAnnotation.class);
        CoNLLUDocumentWriter conllUWriter = new ExtendedCoNLLUDocumentWriter();
        for (CoreMap sentence : sentences) {
            SemanticGraph sg = sentence.get(SemanticGraphCoreAnnotations.BasicDependenciesAnnotation.class);
            if (sg != null) {
                result.append(conllUWriter.printSemanticGraph(sg)).append("\n");
            }
        }
        return result.toString();
    }

    /**
     * Parse input file(s)
     * @param inputFiles
     * @param outputDir
     * @throws IOException
     */
    public void parse(List<File> inputFiles, String outputDir) throws IOException, SQLException, ClassNotFoundException {
        // override existing pipeline
        if (!props.containsKey("outputDirectory")) {
            props.setProperty("outputDirectory", outputDir);
            this.pipeline = new IndonesianStanfordCoreNLP(this.props, this.annotatorPool);
        }
        pipeline.processFiles(inputFiles);
    }

    public static void main(String args[]) {

        // parse arguments
        ArgumentParser parser = ArgumentParsers.newArgumentParser("DependencyParser").defaultHelp(true).description("Generate CONLL-U dependency tree from Indonesian text");
        parser.addArgument("-t", "--text").help("Text input to parse");
        parser.addArgument("-f", "--file").nargs("*").help("File input to parse");
        parser.addArgument("-o", "--outputDir").setDefault(".").help("Output directory");

        Namespace ns = null;
        try {
            ns = parser.parseArgs(args);
        } catch (ArgumentParserException e) {
            parser.handleError(e);
            System.exit(1);
        }

        String text = ns.getString("text");
        List<String> files = ns.<String> getList("file");
        String outputDir = ns.getString("outputDir");
        try {
            if (text != null) {
                System.out.println(new DependencyParser().parse(text.trim()));
            } else if (files != null) {
                List<File> fileList = new ArrayList<>();
                List<String> outputFiles = new ArrayList<>();
                String sep = System.getProperty("file.separator");
                for (String file:files) {
                    File fileObj = new File(file);
                    fileList.add(fileObj);
                    outputFiles.add(outputDir + sep + fileObj.getName() + "." + OUTPUT_FORMAT);
                }
                new DependencyParser().parse(fileList, outputDir);
                System.out.println("File(s) created:");
                for (String outputFile:outputFiles) {
                    System.out.println(outputFile);
                }
            } else {
                System.err.println("No input provided");
            }
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
\end{lstlisting}


%-----------------------------------------------------------------------------%
\addChapter{Lampiran 3: Kode Sumber Pustaka Utama}
%-----------------------------------------------------------------------------%

\chapter*{Lampiran 3: Kode Sumber Pustaka Utama}

Kode sumber pustaka utama (\textit{main library}) yang berisi kode sumber untuk \textit{Triple Candidates Generator}, \textit{Token Expander} dan \textit{Triple Selector} \verb|tripletools.py|

\begin{lstlisting}[language=Python]
import itertools
import csv
import argparse


BEST_FEATURES = [0, 1, 2, 3, 5, 6, 10, 11, 12, 14, 17, 18, 19, 20, 21, 22, 23]  # F1 0.586
# BEST_FEATURES = [0, 1, 2, 3, 5, 6, 10, 11, 12, 17, 18, 19, 20, 21, 22, 23]   # F1 0.579
# BEST_FEATURES = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]  # F1 0.547


# constants
conllu = ['ID', 'FORM', 'LEMMA', 'UPOSTAG', 'XPOSTAG', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC']
postag = ['', 'ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'CONJ']
deprel = ['', 'acl', 'advcl', 'advmod', 'amod', 'appos', 'aux', 'case', 'cc', 'ccomp', 'clf', 'compound', 'conj', 'cop', 'csubj', 'dep', 'det', 'discourse', 'dislocated', 'expl', 'fixed', 'flat', 'goeswith', 'iobj', 'list', 'mark', 'nmod', 'nsubj', 'nummod', 'obj', 'obl', 'orphan', 'parataxis', 'punct', 'reparandum', 'root', 'vocative', 'xcomp', 'nsubjpass', 'name', 'dobj', 'neg', 'mwe', 'csubjpass']
entity = ['', 'PERSON', 'LOCATION', 'ORGANIZATION', 'TIME', 'QUANTITY', 'OTHER']

# extraction RULES
subject_object_candidates_pos = ['PROPN', 'NOUN', 'PRON', 'VERB']
predicate_candidates_pos = ['VERB', 'AUX']
non_subject_object_candidates_form = ['yang', 'adalah']
non_predicate_candidates_form = ['yang']
num_siblings = 1  # bigram


def extract_triples_by_root_children(conllu_s, header):
    """
    Extract features (triples) for clustering from sentence (conllu_s)
    by combining sentence root/header with 2 of its children
    """
    # find all direct branches of header
    direct_branches = []
    for id, row in conllu_s.iteritems():
        # children (direct branches of header)
        if (
            row['head'] == header['id'] and
            row['upostag'] in subject_object_candidates_pos and
            row['form'] not in non_subject_object_candidates_form
        ):
            direct_branches.append(id)

    # yield triples combinations
    if len(direct_branches) > 1:
        for combi in itertools.combinations(direct_branches, 2):
            first = None
            third = None
            if combi[0] < header['id'] and header['id'] < combi[1]:
                first = conllu_s[combi[0]]
                third = conllu_s[combi[1]]
            elif combi[1] < header['id'] and header['id'] < combi[0]:
                first = conllu_s[combi[1]]
                third = conllu_s[combi[0]]

            if first and third:
                second = conllu_s[header['id']]
                yield (first, second, third)


def extract_triples_by_combinations(conllu_s, header):
    """
    Extract features (triples) for clustering from sentence (conllu_s)
    by enumerating all possible triple combination of word
    """
    num_tokens = len(conllu_s)
    # sentence start from 1
    for i in range(1, num_tokens - 2):
        first = conllu_s[i]
        # RULES for Subject
        if (
            first['upostag'] in subject_object_candidates_pos and
            first['form'] not in non_subject_object_candidates_form and
            (first['deprel'] not in ['compound', 'name'] or first['head_distance'] > 2)
        ):
            for j in range(i + 1, num_tokens - 1):
                second = conllu_s[j]
                # RULES for Predicate
                if (
                    second['upostag'] in predicate_candidates_pos
                ):
                    for k in range(j + 1, num_tokens):
                        third = conllu_s[k]
                        # RULES for Object
                        if (
                            third['upostag'] in subject_object_candidates_pos and
                            third['form'] not in non_subject_object_candidates_form and
                            (third['deprel'] not in ['compound', 'name'] or third['head_distance'] > 2) and
                            (third['upostag'] not in predicate_candidates_pos or first['upostag'] not in predicate_candidates_pos)
                        ):
                            s = first['flatten_s']
                            p = second['flatten_p']
                            if third['nearest_adp_id']:
                                p += ' ' + conllu_s[third['nearest_adp_id']]['form']
                            o = third['flatten_o']
                            yield (first, second, third, s, p, o)


def extract_triples_by_children_combination(conllu_s, header):
    """
    Extract features (triples) for clustering from sentence (conllu_s)
    by combining sentence predicate nodes with 2 of their children
    """
    for k, v in conllu_s.items():
        # RULES for Subject, Predicate and Object
        if (
            v['upostag'] in predicate_candidates_pos and
            v['form'] not in non_predicate_candidates_form
        ):
            for first, second, third in extract_triples_by_root_children(conllu_s, v):
                yield (first, second, third)


def trace_children_pos(child_pos_list, parent_pos, node, s):
    """
    Find parent that has parent_pos pos tag and has one child of child_pos
    """
    parent_pos_list = [parent_pos] if parent_pos not in ['NOUN', 'PROPN'] else ['NOUN', 'PROPN']
    if node['deprel'] == 'root' or node['upostag'] not in parent_pos_list:
        return None
    else:
        # find child with upostag == child_pos
        for child_id in node['children']:
            if s[child_id]['upostag'] in child_pos_list:
                return s[child_id]

        # if not found try to search on node's parent
        return trace_children_pos(child_pos_list, parent_pos, s[node['head']], s)


def remove_token_if_first(field, values, tokens):
    while (tokens and tokens[0][1][field] in values):
        tokens.pop(0)


def remove_token_if_last(field, values, tokens):
    while (tokens and tokens[-1][1][field] in values):
        tokens.pop(-1)


def remove_token_if_first_or_last(field, values, tokens):
    remove_token_if_first(field, values, tokens)
    remove_token_if_last(field, values, tokens)


def expand_node(node, s):
    """
    Expand node to its children as dict
    """
    expanded = {node['id']: node}
    has_quote = False
    # EXPAND RULES

    for k in node['children']:
        v = s[k]
        if v['deprel'] in ['compound', 'name', 'amod']:
            expanded.update(expand_node(v, s))
        elif v['entity'] and v['entity'] == node['entity'] and abs(v['id'] - node['id']) == 1:
            expanded.update(expand_node(v, s))
        elif has_quote:
            expanded.update(expand_node(v, s))
        elif node['deprel'] == 'root':     # [Sembungan adalah sebuah] (desa) [.]
            continue
        else:
            if v['form'] in ['\'', '"']:  # (" Lelaki dan Telaga ")
                has_quote = True
            if (v['upostag'] in ['CONJ'] or v['form'] in [',', '/']):  # (kecamatan) Kejajar [, kabupaten Wonosobo]
                break
            if v['upostag'] in ['VERB', 'ADP']:  # (helm) Brodie [yang dipakai]
                continue
            if v['children'] and 'ADP' in [s[i]['upostag'] for i in v['children']]:  # (Stahlhelm) Jerman [dengan perbaikan desain], [Beberapa bulan sebelum] (Rose)
                continue
            expanded.update(expand_node(v, s))

    return expanded


def flatten_node(node, s, expand_as='o', mark_head=False):
    """
    Expand node and its branches to clause string
    """
    if expand_as.lower() in ['s', 'o']:
        expanded = expand_node(node, s)
        sorted_nodes = sorted(expanded.items())

        # EXPAND RULES
        remove_token_if_first_or_last('upostag', ['CONJ', 'ADP'], sorted_nodes)
        remove_token_if_first('form', [')'], sorted_nodes)
        remove_token_if_last('form', ['(', 'yang'], sorted_nodes)

        text = ' '.join([v['form'] if not mark_head or k != node['id'] else '({})'.format(v['form']) for k, v in sorted_nodes])
        ids = [k for k, v in sorted_nodes]
    elif expand_as.lower() in ['p']:
        text = node['form'] if not mark_head else '({})'.format(node['form'])
        ids = [node['id']]

        # EXPAND RULES
        negation_node = [s[c_id] for c_id in node['children'] if s[c_id]['form'].lower() == 'tidak']
        if negation_node:
            text = negation_node[0]['form'] + ' ' + text
            ids = [negation_node[0]['id']] + ids

    return text, ids


def flatten_conllu_sentence(conllu_s):
    return ' '.join([token['form'] for token in conllu_s.values()])


def set_extra_properties(s, children, mark_head=False):
    """
    Retrieve head's pos tag
    Flatten subject/object candidates
    """
    for k, v in s.iteritems():
        # get head pos tag
        s[k]['head_upostag'] = s[v['head']]['upostag'] if v['head'] > 0 else ''
        # get siblings pos tags
        before = v['id'] - num_siblings
        s[k]['before_upostag'] = [s[i]['upostag'] if i > 0 else '' for i in range(before, v['id'])]
        after = v['id'] + num_siblings + 1
        s[k]['before_upostag'] = [s[i]['upostag'] if i < len(s) else '' for i in range(after - num_siblings, after)]
        # get children id
        if k in children:
            sorted_children = sorted(children[k])
            s[k]['children'] = sorted_children

    # loop once more to flatten as children is required
    for k, v in s.iteritems():
        if v['upostag'] in subject_object_candidates_pos:
            s[k]['flatten_s'], s[k]['flatten_s_id'] = flatten_node(s[k], s, expand_as='s', mark_head=mark_head)
            s[k]['flatten_o'], s[k]['flatten_o_id'] = flatten_node(s[k], s, expand_as='o', mark_head=mark_head)
            # trace ADP node to parents to be inherited
            if v['head'] > 0:
                nearest_adp_node = trace_children_pos(['ADP'], v['upostag'], v, s)
                if nearest_adp_node:
                    s[k]['nearest_adp_id'] = nearest_adp_node['id']
        if v['upostag'] == 'VERB':
            s[k]['flatten_p'], s[k]['flatten_p_id'] = flatten_node(s[k], s, expand_as='p', mark_head=mark_head)


def get_neigbour_upostag(position, token):
    key = position + '_upostag'
    if position not in ['before', 'after'] or not token[key]:
        return postag.index('')
    return postag.index(token[key][0])


def get_next_upostag(token):
    return get_neigbour_upostag('after', token)


def get_prev_upostag(token):
    return get_neigbour_upostag('before', token)


def vectorize(first, second, third):
    """
    Convert a triple's member to feature vector
    """
    distance_first_second = abs(first['id'] - second['id'])
    distance_second_third = abs(second['id'] - third['id'])
    first_is_child_of_second = 1 if first['id'] in second['children'] else 0
    third_is_child_of_second = 1 if third['id'] in second['children'] else 0

    vector = []
    vector.append(postag.index(first['upostag']))
    vector.append(deprel.index(first['deprel']))
    vector.append(postag.index(first['head_upostag']))
    vector.append(entity.index(first['entity']))
    vector.append(len(first['children']))
    vector.append(distance_first_second)
    vector.append(first_is_child_of_second)
    vector.append(get_prev_upostag(first))
    vector.append(get_next_upostag(first))
    vector.append(1 if first['nearest_adp_id'] else 0)

    vector.append(postag.index(second['upostag']))
    vector.append(deprel.index(second['deprel']))
    vector.append(postag.index(second['head_upostag']))
    vector.append(entity.index(second['entity']))
    vector.append(len(second['children']))
    vector.append(get_prev_upostag(second))
    vector.append(get_next_upostag(second))

    vector.append(postag.index(third['upostag']))
    vector.append(deprel.index(third['deprel']))
    vector.append(postag.index(third['head_upostag']))
    vector.append(entity.index(third['entity']))
    vector.append(len(third['children']))
    vector.append(distance_second_third)
    vector.append(third_is_child_of_second)
    vector.append(get_prev_upostag(third))
    vector.append(get_next_upostag(third))
    vector.append(1 if third['nearest_adp_id'] else 0)

    return vector


def parse_connlu_file(conllu_file, mark_head=False):
    with open(conllu_file, 'rb') as csvfile:
        reader = csv.reader(csvfile, delimiter='\t', quoting=csv.QUOTE_NONE)
        s = {}
        children = {}
        s_header = None
        index = 0
        for row in reader:
            if len(row) > 0:
                id = int(row[conllu.index('ID')])
                head_id = int(row[conllu.index('HEAD')])
                deprel = row[conllu.index('DEPREL')].split(':')[0]  # ignore sub relation
                obj = {
                    'id': id,
                    'sentence_id': index,
                    'form': row[conllu.index('FORM')],
                    'upostag': row[conllu.index('UPOSTAG')],
                    'head': head_id,
                    'head_distance': abs(head_id - id) if head_id > 0 else 0,
                    'deprel': deprel if deprel != '_' else 'root',
                    'head_upostag': '',
                    'before_upostag': [],
                    'after_upostag': [],
                    'flatten_s': row[conllu.index('FORM')],
                    'flatten_p': row[conllu.index('FORM')],
                    'flatten_o': row[conllu.index('FORM')],
                    'flatten_s_id': [id],
                    'flatten_p_id': [id],
                    'flatten_o_id': [id],
                    'entity': row[conllu.index('MISC')] if row[conllu.index('MISC')] != '_' else '',
                    'children': [],
                    'nearest_adp_id': None
                }
                s[id] = obj
                # map children
                if obj['head'] != 0:
                    if obj['head'] not in children:
                        children[obj['head']] = []
                    if id not in children[obj['head']]:
                        children[obj['head']].append(id)
                # find root header
                s_header = obj if obj['head'] == 0 else s_header
            else:
                set_extra_properties(s, children, mark_head)
                yield index, s, s_header
                s = {}
                index += 1
                children = {}
        if s:
            # if last element not a blank
            set_extra_properties(s, children, mark_head)
            yield index, s, s_header


def get_best_features():
    return BEST_FEATURES
\end{lstlisting}


%-----------------------------------------------------------------------------%
\addChapter{Lampiran 4: Kode Sumber Pelatihan \textit{Triple selector}}
%-----------------------------------------------------------------------------%

\chapter*{Lampiran 4: Kode Sumber Pelatihan \textit{Triple selector}}

Kode sumber pelatihan dan perbandingan \textit{Triple Selector} \verb|classifier.py|

\begin{lstlisting}[language=Python]
import argparse
import collections
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.externals import joblib
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_fscore_support
from tripletools import get_best_features
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches


plt.style.use('ggplot')

experiments = [
    {
        'name': 'Logistic Regression',
        'model': LogisticRegression(),
        'params': [
            {
                'solver': ['liblinear'],
                'penalty': ['l2'],
                'random_state': [77]
            },
        ]
    },
    {
        'name': 'SVM',
        'model': SVC(),
        'params': [
            {
                'kernel': ['poly'],
                'degree': [5],
                'random_state': [77]
            },
        ]
    },
    {
        'name': 'MLP',
        'model': MLPClassifier(max_iter=1000),
        'params': [
            {
                'hidden_layer_sizes': [(20, 10)],
                'random_state': [77]
            }
        ]
    },
    {
        'name': 'Random Forest',
        'model': RandomForestClassifier(),
        'params': [
            {
                'max_depth': [8],
                'n_estimators': [20],
                'min_samples_split': [5],
                'criterion': ['gini'],
                'max_features': ['auto'],
                'class_weight': ['balanced'],
                'random_state': [77]
            }
        ]
    },
]


def cross_validate_precision_recall_fbeta(model, X, y, cv=None):
    precision = cross_val_score(model, X, y, cv=cv, scoring='precision').mean()
    recall = cross_val_score(model, X, y, cv=cv, scoring='recall').mean()
    fbeta_list = cross_val_score(model, X, y, cv=cv, scoring='f1')
    fbeta = fbeta_list.mean()
    fbeta_min = fbeta_list.min()
    fbeta_max = fbeta_list.max()
    fbeta_std = fbeta_list.std()
    return precision, recall, fbeta, fbeta_min, fbeta_max, fbeta_std


def plot_model_performance_comparison(experiments):
    fig, ax = plt.subplots()

    # Example data
    x_data = []
    y_dict = {
        'precision': {'color': '#f9f1c5', 'data': []},
        'recall': {'color': 'lightblue', 'data': []},
        'f1': {'color': 'green', 'data': []},
    }
    for exp in experiments:
        x_data.append(exp['name'])
        y_dict['precision']['data'].append(exp['best_score']['precision'])
        y_dict['recall']['data'].append(exp['best_score']['recall'])
        y_dict['f1']['data'].append(exp['best_score']['f1'])

    x = np.arange(len(x_data))
    width = 0.20
    i = 1
    legend_handles = []
    for label, y in y_dict.items():
        ax.bar(x + width * i, y['data'], width, color=y['color'])
        legend_handles.append(mpatches.Patch(color=y['color'], label=label))
        i += 1
    ax.set_xticks(x + width * 2)
    ax.set_xticklabels(x_data)
    ax.set_yticks(np.arange(0.0, 1.1, 0.1))
    ax.set_title('Triple Selector Models Performance')

    lgd = plt.legend(handles=legend_handles)
    plt.show()

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Train triples classifier')
    parser.add_argument('dataset_path', help='Dataset path')
    parser.add_argument('-o', '--output_path', help='Output model path', default='triples-classifier-model.pkl')
    parser.add_argument('-s', '--scaler_output_path', help='Output scaler path', default='triples-classifier-scaler.pkl')
    parser.add_argument('-b', '--best', help='search parameters that gives best model', action='store_true')
    parser.add_argument('--nocv', help='no cross-validation. training accuracy only', action='store_true')
    args = parser.parse_args()

    # load dataset
    dataset = np.genfromtxt(args.dataset_path, delimiter=',', dtype='float32')
    total_features = dataset.shape[1] - 1

    # feature selection
    selected_features = get_best_features()
    print('Total features: {}'.format(total_features))
    print('Selected features: {} ({})'.format(selected_features, len(selected_features)))

    X = dataset[:, selected_features]
    y = dataset[:, -1]
    scaler = StandardScaler().fit(X)
    X = scaler.transform(X)
    joblib.dump(scaler, args.scaler_output_path)

    # collect dataset statistics
    counter = collections.Counter(y)
    print(counter)
    pos = counter[1] * 1.0 / (counter[0] + counter[1])
    neg = 1.0 - pos

    # exhaustive best parameters search
    cv = None
    print('')
    if args.best:
        best_score = 0.0
        best_model = None
        count = 0
        for experiment in experiments:
            search = GridSearchCV(
                estimator=experiment['model'],
                param_grid=experiment['params'],
                scoring='f1',
                cv=cv
            )
            search.fit(X, y)
            if args.nocv:
                y_pred = search.best_estimator_.predict(X)
                precision, recall, fbeta, support = precision_recall_fscore_support(y, y_pred, average='binary')
                fbeta_min = fbeta_max = fbeta_std = fbeta
            else:
                precision, recall, fbeta, fbeta_min, fbeta_max, fbeta_std = cross_validate_precision_recall_fbeta(search.best_estimator_, X, y)
            print(search.best_estimator_)
            print('Precision: {}\nRecall: {}\nF1 avg: {}\nF1 min: {}\nF1 max: {}\nF1 std: {}\n'.format(
                precision,
                recall,
                fbeta,
                fbeta_min,
                fbeta_max,
                fbeta_std,
            ))
            experiment['best_model'] = best_model
            experiment['best_score'] = {'precision': precision, 'recall': recall, 'f1': fbeta}
            # replace current best model if the score is higher
            if search.best_score_ > best_score:
                best_score = search.best_score_
                best_model = search.best_estimator_
            count += 1
        print('--------------- Result ----------------')
        print('Best models: {} (F1 = {})'.format(best_score, type(best_model).__name__))
        model = best_model

        # show plot
        plot_model_performance_comparison(experiments)

    else:
        model = RandomForestClassifier(max_depth=8, class_weight='balanced', n_estimators=20, min_samples_split=5, max_features='auto', random_state=77)

        # cross validate best model to compare score
        precision, recall, fbeta, fbeta_min, fbeta_max, fbeta_std = cross_validate_precision_recall_fbeta(model, X, y)
        print('Precision: {}\nRecall: {}\nF1 avg: {}\nF1 min: {}\nF1 max: {}\nF1 std: {}\n'.format(
            precision,
            recall,
            fbeta,
            fbeta_min,
            fbeta_max,
            fbeta_std,
        ))

    # save model to file
    joblib.dump(model, args.output_path)
    print('Model saved to {}'.format(args.output_path))
    print('Scaler saved to {}'.format(args.scaler_output_path))
\end{lstlisting}

%-----------------------------------------------------------------------------%
\addChapter{Lampiran 5: Daftar \textit{POS Tag} dan \textit{Dependency Relation} CoNLL-U}
%-----------------------------------------------------------------------------%

\chapter*{Lampiran 5: Daftar \textit{POS Tag} dan \textit{Dependency Relation} CoNLL-U}

\textbf{\textit{POS tag}}

\begin{multicols}{2}
	\begin{enumerate}
		\item ADJ: \textit{adjective}
		\item ADP: \textit{adposition}
		\item ADV: \textit{adverb}
		\item AUX: \textit{auxiliary}
		\item CCONJ: \textit{coordinating conjunction}
		\item DET: \textit{determiner}
		\item INTJ: \textit{interjection}
		\item NOUN: \textit{noun}
		\item NUM: \textit{numeral}
		\item PART: \textit{particle}
		\item PRON: \textit{pronoun}
		\item PROPN: \textit{proper noun}
		\item PUNCT: \textit{punctuation}
		\item SCONJ: \textit{subordinating conjunction}
		\item SYM: \textit{symbol}
		\item VERB: \textit{verb}
		\item X: \textit{other}
	\end{enumerate}
\end{multicols}

\noindent\textbf{\textit{Dependency Relation}}

\begin{multicols}{2}
	\begin{enumerate}
		\item acl: \textit{clausal modifier of noun (adjectival clause)}
		\item advcl: \textit{adverbial clause modifier}
		\item advmod: \textit{adverbial modifier}
		\item amod: \textit{adjectival modifier}
		\item appos: \textit{appositional modifier}
		\item aux: \textit{auxiliary}
		\item case: \textit{case marking}
		\item cc: \textit{coordinating conjunction}
		\item ccomp: \textit{clausal complement}
		\item clf: \textit{classifier}
		\item compound: \textit{compound}
		\item conj: \textit{conjunct}
		\item cop: \textit{copula}
		\item csubj: \textit{clausal subject}
		\item dep: \textit{unspecified dependency}
		\item det: \textit{determiner}
		\item discourse: \textit{discourse element}
		\item dislocated: \textit{dislocated elements}
		\item expl: \textit{expletive}
		\item fixed: \textit{fixed multiword expression}
		\item flat: \textit{flat multiword expression}
		\item goeswith: \textit{goes with}
		\item iobj: \textit{indirect object}
		\item list: \textit{list}
		\item mark: \textit{marker}
		\item nmod: \textit{nominal modifier}
		\item nsubj: \textit{nominal subject}
		\item nummod: \textit{numeric modifier}
		\item obj: \textit{object}
		\item obl: \textit{oblique nominal}
		\item orphan: \textit{orphan}
		\item parataxis: \textit{parataxis}
		\item punct: \textit{punctuation}
		\item reparandum: \textit{overridden disfluency}
		\item root: \textit{root}
		\item vocative: \textit{vocative}
		\item xcomp: \textit{open clausal complement}
	\end{enumerate}
\end{multicols}
